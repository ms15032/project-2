# Project 2

All project-related code is in `model.ipynb`.  It was run in a Kaggle notebook environment with a `P100` GPU.

## Files

* `model.ipynb` - all code including training and evaluation
* `project.pdf` - the project brief and instructions from the instructor
* `report` - all report-related files
  * `main.tex` - LaTeX file for the project report
  * `aaai24.bib` - report references file
  * `aaai24.sty` - report styling file
  * `aaai24.bst` - report references styling file
  * `*.png` - figures for the report 
* `report.pdf` - final project report generated from LaTeX

## Attributions

* Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019, July 26). ROBERTA: A robustly optimized BERT pretraining approach. arXiv.org. https://arxiv.org/abs/1907.11692
* Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021, June 17). LORA: Low-Rank adaptation of Large Language Models. arXiv.org. https://arxiv.org/abs/2106.09685
